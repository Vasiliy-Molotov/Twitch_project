{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import threading\n",
    "import re\n",
    "import codecs\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#настройки показа в панде\n",
    "pd.options.display.max_columns = 100 #def100\n",
    "pd.options.display.max_colwidth = 100 #def100\n",
    "pd.options.display.max_rows = 200 #def 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# время работы скрипта\n",
    "# лимит обращений отсутствует\n",
    "# время - каждые через ровные 10 минут, чтобы ровно по сетке, начиная с 00:00\n",
    "\n",
    "# get streams' summary\n",
    "# https://api.twitch.tv/kraken/streams/summary\n",
    "\n",
    "# результат - статья на хабре, данные для стримеров\n",
    "\n",
    "# № users, that prefer you on, and prefer other on you\n",
    "\n",
    "# размер получаемых данных можно уменьшить, убрав ненужное\n",
    "\n",
    "# next step - look data on followers\n",
    "\n",
    "# parallel requests\n",
    "\n",
    "# working on and with missing data\n",
    "\n",
    "\n",
    "# содержание, оглавление и закладки для jupiter, чтобы можно было работать с большими файлами\n",
    "\n",
    "# что за двойное \";;\" при сохранение пандой в csv?\n",
    "# меняет ли excel кодировку? \n",
    "# пока загружается не все при чем по неизвестным причинам (попробовать сохранить с другим разделителем?!)\n",
    "\n",
    "\n",
    "# todo:\n",
    "# 1) проверить, что все файлы сохранились в кодировке utf-8\n",
    "# 2) сделать скрипт загрузки в mysql\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# на что может влиять стример - расписание, игра, кооперация с другими стримерами\n",
    "# таргетом могут быть не раскрученые канала, а раскручивающиеся с аналитикой кто от них, когда и почему ушел\n",
    "# но тогда нужно собирать больше данных\n",
    "\n",
    "# connector only for python 3.4\n",
    "\n",
    "\n",
    "\n",
    "# собрать большое количество данных на специально подобранную инфраструктуру,\n",
    "# это уже объективный барьер и потому конкурентное преимущество\n",
    "# иметь данные - это потенциал\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#загрузка csv в Mysql\n",
    "\n",
    "# опция -forced почему-то работает не всегда, точнее для тех ошибок,\n",
    "# которые были встречены на данный момент, она вообще не работает\n",
    "\n",
    "# название игры, включающее \";\" не проблема для загрузки csv в mysql. хотя отоброжается как несколько полей в xlsx\n",
    "\n",
    "# иногда число просмотров канало отражается как \"\", вместо \"0\", тогда импорт csv в mysql не происходил из-за ошибки\n",
    "# нужно менять \"\" на \"0\"\n",
    "\n",
    "# есть некоторый дубляж при загрузке данных, когда один и тотже стрим попадает на разные оффсеты\n",
    "# например, в 2015_12_17___18_00.csv,\n",
    "# стрим от oushu_genji попал в 2 офсета c 666 и 679 зрителей соответственно:\n",
    "# https://api.twitch.tv/kraken/streams?limit=100&offset=100\n",
    "# https://api.twitch.tv/kraken/streams?limit=100&offset=0\n",
    "\n",
    "# ошибка, если \"\\\" встречается в названии игры или где-то еще из-за опции \"enclosed by \"\\\"\"\n",
    "# при импорте через wizzard все нормально загружается\n",
    "# нужна ли вообще опции \"enclosed by?\"\n",
    "# нет, нельзя, т.к. запятые будут лишние, например в \"Warhammer 40,000: Dawn of War II - Retribution\"\n",
    "# можно ли сделать, чтобы mysql при загрузке не воспринимал \"\\\" как особый символ?\n",
    "# Да, нужно дописать в команду \"ESCAPED BY ''\" после \"enclosed by ...\"\n",
    "\n",
    "# название игры, вроде может отсутствовать, и это будет ок для импорта\n",
    "\n",
    "\n",
    "# долгие поля mysql не воспринимает\n",
    "# например, для такого названия, содержащего зачем-то еще и лишнюю инфу\n",
    "# \"The Bureau: XCOM Declassified (32-bit, DX9) - D:\\Steam\\steamapps\\common\\The Bureau\\binaries\\win32\\\"\n",
    "\n",
    "\n",
    "\n",
    "# also it would be good to gather all games summary from twitch\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "# погрешность из-за того, что некоторые каналы дублируюца в одном дампе (2% # просмотров для одного дампа)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "way = 'G:\\!datascience\\twitch\\\\'\n",
    "way1 = r'G:\\\\!datascience\\twitch\\\\'\n",
    "way2 = r'G:\\\\!datascience\\twitch\\parsed\\\\'\n",
    "way3 = r'G:\\\\!datascience\\twitch\\reserved\\\\'\n",
    "way4 = r'G:\\\\!datascience\\twitch\\load\\\\'\n",
    "way10 =  'G:\\!datascience\\\\twitch\\\\analytics\\games_split1.csv'\n",
    "way11 = 'G:\\!datascience\\\\twitch\\\\analytics\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Get stream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# twitch requires headers in requests\n",
    "\n",
    "global headers\n",
    "headers = {'client_id': 'oub6145j9kw161j9wnq6ayrcfnti3hw',\n",
    "           'Accept': 'application/vnd.twitchtv.v3+json',\n",
    "           'content-type': 'application/json',\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_chat_data(chan_n):\n",
    "    adr_chat = 'https://tmi.twitch.tv/group/user/'\n",
    "    ch_ct_add = {}\n",
    "    ch_ct_add['chatters'] = []\n",
    "    ch_ct_add['status'] = ''  \n",
    "    ch_ct_add['try_number'] = 0\n",
    "    nt = 3 # number of tries\n",
    "    gct_sleep = 0.1 # sleep time if error and try again\n",
    "    while nt > 0:\n",
    "        nt+=-1\n",
    "        ch_ct_add['try_number']+= 1      \n",
    "        req = requests.get(adr_chat + chan_n + '/chatters')\n",
    "        if req.status_code == 200:\n",
    "            ch_ct_add['chatters'] = req.json()\n",
    "            ch_ct_add['status'] = 'collected'    \n",
    "            ch_ct_add['channel'] = chan_n\n",
    "            return(ch_ct_add)\n",
    "        else:\n",
    "            time.sleep(gct_sleep)\n",
    "            ch_ct_add['status'] = 'error'\n",
    "    return(ch_ct_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stream_list(adr):\n",
    "    max_nt_stl = 3 # number of tries to get data\n",
    "    curr_nt_stl = 0\n",
    "    stl_get_status = 0\n",
    "    while (stl_get_status == 0) and (curr_nt_stl < max_nt_stl):\n",
    "        req =  requests.get(adr, headers)\n",
    "        curr_nt_stl+= 1\n",
    "        if req.status_code == 200:\n",
    "            stl_get_status = 1 # received\n",
    "            stl_raw = req.json()\n",
    "        else:\n",
    "            stl_get_status = 0 # failed to receive\n",
    "            stl_raw = []\n",
    "            time.sleep(stL_raw_sleep_time)\n",
    "    data = {}\n",
    "    data['adr'] = adr\n",
    "    data['status'] = stl_get_status\n",
    "    data['number_tries'] = curr_nt_stl\n",
    "    data['streams'] = stl_raw\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_start = time.ctime()\n",
    "time_start_short = str(time.strftime('%Y_%m_%d___%H_%M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the summary and the list of all active streams (channels)\n",
    "\n",
    "adr_summary = 'https://api.twitch.tv/kraken/streams/summary'\n",
    "tw_summary = requests.get(adr_summary, headers).json()\n",
    "ttlvn = tw_summary['viewers'] # total viewers now  \n",
    "ttlv_pt = 0 # progressive total for viewers count\n",
    "stL_raw_sleep_time = 1 # time between tries to get stl_raw data in case of connection error\n",
    "stl_get_const = 3 # assumptionL: no more than 3*100 = 300 can be started during the gata gathering\n",
    "\n",
    "adr_list = ['https://api.twitch.tv/kraken/streams?limit=100&offset=' + str(i*100) for i in range(int(tw_summary['channels']/100) + stl_get_const)]\n",
    "\n",
    "pool = ThreadPool(100) \n",
    "streams = pool.map(get_stream_list, adr_list)\n",
    "pool.close() \n",
    "pool.join() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the list of top most viewed channels having 90% of viewers in total\n",
    "\n",
    "viewers_cut_const = 0.9 # get channel and chat data only for top channels having 90% of all twitch viewers in total atm\n",
    "k = 0 \n",
    "ttlv_pt = 0\n",
    "gcsl = [] # get chat stream list\n",
    "for i in range(len(streams)):\n",
    "    for j in range(len(streams[i]['streams']['streams'])):\n",
    "        k+=1\n",
    "        ttlv_pt+= streams[i]['streams']['streams'][j]['viewers']\n",
    "        if ttlv_pt/ttlvn <= viewers_cut_const:\n",
    "            gcsl.append(streams[i]['streams']['streams'][j]['channel']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get chat data for 90% viewers\n",
    "\n",
    "pool = ThreadPool(100) \n",
    "ch_ct = pool.map(get_chat_data, gcsl)\n",
    "pool.close() \n",
    "pool.join() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count errors\n",
    "\n",
    "chat_error = 0\n",
    "for i in range(len(ch_ct)):\n",
    "    if ch_ct[i]['status'] != 'collected':\n",
    "        chat_error+=1\n",
    "\n",
    "stream_batch_error = 0\n",
    "for i in range(len(streams)):\n",
    "    if streams[i]['status'] != 1:\n",
    "        stream_batch_error+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# structuring the resulting file in a json format\n",
    "time_end = time.ctime()\n",
    "dump_file ={}\n",
    "log = {}\n",
    "time_info = {}\n",
    "time_info['time_start'] = time_start\n",
    "time_info['time_end'] = time_end\n",
    "log['time']=time\n",
    "error_count = {}\n",
    "error_count['stream_batch'] = stream_batch_error\n",
    "error_count['chat'] = chat_error\n",
    "log['error_count'] = error_count\n",
    "\n",
    "dump_file['summary'] = tw_summary\n",
    "dump_file['streams'] = streams\n",
    "dump_file['ch_ct'] = ch_ct\n",
    "dump_file['log'] = time_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final writing of gathred data\n",
    "with open(time_start_short +str('__v3'), 'w') as outfile:\n",
    "    json.dump(dump_file, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import gridfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # tutorial stuff\n",
    "# db = client['primer']\n",
    "# coll = db['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = client['Twitch1']\n",
    "collection = db['Twitch1_import1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get list of files in a directory\n",
    "# script\n",
    "dirp = os.listdir(way1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# script\n",
    "# read a json file\n",
    "\n",
    "f1 =[]\n",
    "\n",
    "with open(way1+new_files[0]) as f:\n",
    "    for line in f:\n",
    "        f1.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016_01_15___21_50__v3'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirp[3300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_files = re.findall('\\w*__v3', '|'.join(dirp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016_01_05___23_00__v3'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectId('57f94fcf583a652494c2c9e3')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.insert_one(f1[0]['summary']).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'Twitch1'), 'Twitch1_import1')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_links': {'self': 'https://api.twitch.tv/kraken/streams/summary'},\n",
       " 'channels': 24780,\n",
       " 'viewers': 945794}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1[0]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('57f94fcf583a652494c2c9e3'),\n",
       " '_links': {'self': 'https://api.twitch.tv/kraken/streams/summary'},\n",
       " 'channels': 24780,\n",
       " 'viewers': 945794}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic = {}\n",
    "dic['1']=f1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "DocumentTooLarge",
     "evalue": "BSON document too large (58755513 bytes) - the connected server supports BSON document sizes up to 16793598 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDocumentTooLarge\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-3c2f6546b201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minserted_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\pymongo\\collection.py\u001b[0m in \u001b[0;36minsert_one\u001b[0;34m(self, document, bypass_document_validation)\u001b[0m\n\u001b[1;32m    628\u001b[0m             return InsertOneResult(\n\u001b[1;32m    629\u001b[0m                 self._insert(sock_info, document,\n\u001b[0;32m--> 630\u001b[0;31m                              bypass_doc_val=bypass_document_validation),\n\u001b[0m\u001b[1;32m    631\u001b[0m                 self.write_concern.acknowledged)\n\u001b[1;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\pymongo\\collection.py\u001b[0m in \u001b[0;36m_insert\u001b[0;34m(self, sock_info, docs, ordered, check_keys, manipulate, write_concern, op_id, bypass_doc_val)\u001b[0m\n\u001b[1;32m    533\u001b[0m             return self._insert_one(\n\u001b[1;32m    534\u001b[0m                 \u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m                 check_keys, manipulate, write_concern, op_id, bypass_doc_val)\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\pymongo\\collection.py\u001b[0m in \u001b[0;36m_insert_one\u001b[0;34m(self, sock_info, doc, ordered, check_keys, manipulate, write_concern, op_id, bypass_doc_val)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0mcodec_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__write_response_codec_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 check_keys=check_keys)\n\u001b[0m\u001b[1;32m    517\u001b[0m             \u001b[0m_check_write_command_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\pymongo\\pool.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[1;31m# Catch socket.error, KeyboardInterrupt, etc. and close ourselves.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_connection_failure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_doc_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\pymongo\\pool.py\u001b[0m in \u001b[0;36m_raise_connection_failure\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0m_raise_connection_failure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\pymongo\\pool.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern)\u001b[0m\n\u001b[1;32m    237\u001b[0m                            \u001b[0mcheck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowable_errors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                            \u001b[0mcheck_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlisteners\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_bson_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                            read_concern)\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOperationFailure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\pymongo\\network.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(sock, dbname, spec, slave_ok, is_mongos, read_preference, codec_options, check, allowable_errors, address, check_keys, listeners, max_bson_size, read_concern)\u001b[0m\n\u001b[1;32m     85\u001b[0m             and size > max_bson_size + message._COMMAND_OVERHEAD):\n\u001b[1;32m     86\u001b[0m         message._raise_document_too_large(\n\u001b[0;32m---> 87\u001b[0;31m             name, size, max_bson_size + message._COMMAND_OVERHEAD)\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpublish\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\pymongo\\message.py\u001b[0m in \u001b[0;36m_raise_document_too_large\u001b[0;34m(operation, doc_size, max_size)\u001b[0m\n\u001b[1;32m    612\u001b[0m                                \u001b[1;34m\" - the connected server supports\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                                \u001b[1;34m\" BSON document sizes up to %d\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                                \" bytes.\" % (doc_size, max_size))\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[1;31m# There's nothing intelligent we can say\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocumentTooLarge\u001b[0m: BSON document too large (58755513 bytes) - the connected server supports BSON document sizes up to 16793598 bytes."
     ]
    }
   ],
   "source": [
    "collection.insert_one(dic).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = gridfs.GridFS(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only write strings or file-like objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\gridfs\\grid_file.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[1;31m# file-like\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mread\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-652923656c3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\gridfs\\__init__.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mgrid_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridIn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__collection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mgrid_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mgrid_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Anaconda3\\lib\\site-packages\\gridfs\\grid_file.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[1;31m# string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"can only write strings or file-like objects\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only write strings or file-like objects"
     ]
    }
   ],
   "source": [
    "a = fs.put(f1[0], encoding = 'utf-8' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'G:\\\\\\\\!datascience\\\\twitch\\\\\\\\2016_01_05___23_00__v3'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.get(a).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method put in module gridfs:\n",
      "\n",
      "put(data, **kwargs) method of gridfs.GridFS instance\n",
      "    Put data in GridFS as a new file.\n",
      "    \n",
      "    Equivalent to doing::\n",
      "    \n",
      "      try:\n",
      "          f = new_file(**kwargs)\n",
      "          f.write(data)\n",
      "      finally:\n",
      "          f.close()\n",
      "    \n",
      "    `data` can be either an instance of :class:`str` (:class:`bytes`\n",
      "    in python 3) or a file-like object providing a :meth:`read` method.\n",
      "    If an `encoding` keyword argument is passed, `data` can also be a\n",
      "    :class:`unicode` (:class:`str` in python 3) instance, which will\n",
      "    be encoded as `encoding` before being written. Any keyword arguments\n",
      "    will be passed through to the created file - see\n",
      "    :meth:`~gridfs.grid_file.GridIn` for possible arguments. Returns the\n",
      "    ``\"_id\"`` of the created file.\n",
      "    \n",
      "    If the ``\"_id\"`` of the file is manually specified, it must\n",
      "    not already exist in GridFS. Otherwise\n",
      "    :class:`~gridfs.errors.FileExists` is raised.\n",
      "    \n",
      "    :Parameters:\n",
      "      - `data`: data to be written as a file.\n",
      "      - `**kwargs` (optional): keyword arguments for file creation\n",
      "    \n",
      "    .. versionchanged:: 3.0\n",
      "       w=0 writes to GridFS are now prohibited.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fs.put)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
